{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASR Assignment 2022-23\n",
    "\n",
    "This notebook has been provided as a template to get you started on the assignment.  Feel free to use it for your development, or do your development directly in Python.\n",
    "\n",
    "You can find a full description of the assignment [here](http://www.inf.ed.ac.uk/teaching/courses/asr/2022-23/coursework.pdf).\n",
    "\n",
    "You are provided with two Python modules `observation_model.py` and `wer.py`.  The first was described in [Lab 3](https://github.com/ZhaoZeyu1995/asr_labs/blob/master/asr_lab3_4.ipynb).  The second can be used to compute the number of substitution, deletion and insertion errors between ASR output and a reference text.\n",
    "\n",
    "It can be used as follows:\n",
    "\n",
    "```python\n",
    "import wer\n",
    "\n",
    "my_refence = 'A B C'\n",
    "my_output = 'A C C D'\n",
    "\n",
    "wer.compute_alignment_errors(my_reference, my_output)\n",
    "```\n",
    "\n",
    "This produces a tuple $(s,d,i)$ giving counts of substitution,\n",
    "deletion and insertion errors respectively - in this example (1, 0, 1).  The function accepts either two strings, as in the example above, or two lists.  Matching is case sensitive.\n",
    "\n",
    "## Template code\n",
    "\n",
    "Assuming that you have already made a function to generate an WFST, `create_wfst()` and a decoder class, `MyViterbiDecoder`, you can perform recognition on all the audio files as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import wer\n",
    "import observation_model\n",
    "import openfst_python as fst\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_type = 'log'\n",
    "# weight_type = 'tropical'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import check_call\n",
    "from IPython.display import Image\n",
    "def draw_f(f):\n",
    "    f.draw('tmp.dot', portrait=True)\n",
    "    check_call(['dot','-Tpng','-Gdpi=200','tmp.dot','-o','tmp.png'])\n",
    "    return Image(filename='tmp.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_file(file: str, txt: str):\n",
    "    f = open(file, 'a')\n",
    "    f.write(txt)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_lexicon(lex_file):\n",
    "    \"\"\"\n",
    "    Parse the lexicon file and return it in dictionary form.\n",
    "    \n",
    "    Args:\n",
    "        lex_file (str): filename of lexicon file with structure '<word> <phone1> <phone2>...'\n",
    "                        eg. peppers p eh p er z\n",
    "\n",
    "    Returns:\n",
    "        lex (dict): dictionary mapping words to list of phones\n",
    "    \"\"\"\n",
    "    \n",
    "    lex = {}  # create a dictionary for the lexicon entries\n",
    "    with open(lex_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.split()  # split at each space\n",
    "            lex[line[0]] = line[1:]  # first field the word, the rest is the phones\n",
    "    return lex\n",
    "\n",
    "lex = parse_lexicon('lexicon.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_symbol_tables(lexicon, n=3):\n",
    "    '''\n",
    "    Return word, phone and state symbol tables based on the supplied lexicon\n",
    "    \n",
    "    Args:\n",
    "        lexicon (dict): lexicon to use, created from the parse_lexicon() function\n",
    "        n (int): number of states for each phone HMM\n",
    "        \n",
    "    Returns:\n",
    "        word_table (fst.SymbolTable): table of words\n",
    "        phone_table (fst.SymbolTable): table of phones\n",
    "        state_table (fst.SymbolTable): table of HMM phone-state IDs\n",
    "    '''\n",
    "    state_table = fst.SymbolTable()\n",
    "    phone_table = fst.SymbolTable()\n",
    "    word_table = fst.SymbolTable()\n",
    "    \n",
    "    # your code here\n",
    "    # .add_symbol('<eps>')\n",
    "    word_table.add_symbol('<eps>')\n",
    "    for word in lexicon:\n",
    "        word_table.add_symbol(word)\n",
    "        \n",
    "    phone_table.add_symbol('<eps>')\n",
    "    for word in list(lexicon):\n",
    "        for phone in lexicon[word]:\n",
    "            phone_table.add_symbol(phone)\n",
    "    \n",
    "    state_table.add_symbol('<eps>')\n",
    "    for word in list(lexicon):\n",
    "        for phone in lexicon[word]:\n",
    "            for i in range(n):\n",
    "                state_table.add_symbol(f\"{phone}_{i+1}\")\n",
    "    \n",
    "\n",
    "    return word_table, phone_table, state_table\n",
    "\n",
    "word_table, phone_table, state_table = generate_symbol_tables(lex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_phone_wfst(f, start_state, phone, n, word = '', isLast=False):\n",
    "    \"\"\"\n",
    "    Generate a WFST representing an n-state left-to-right phone HMM.\n",
    "    \n",
    "    Args:\n",
    "        f (fst.Fst()): an FST object, assumed to exist already\n",
    "        start_state (int): the index of the first state, assumed to exist already\n",
    "        phone (str): the phone label \n",
    "        n (int): number of emitting states of the HMM\n",
    "        \n",
    "    Returns:\n",
    "        the final state of the FST\n",
    "    \"\"\"\n",
    "    \n",
    "    current_state = start_state\n",
    "    eps = phone_table.find('<eps>')\n",
    "    out = word_table.find(word)\n",
    "    \n",
    "    for i in range(1, n+1):\n",
    "    \n",
    "        in_label = state_table.find('{}_{}'.format(phone, i))\n",
    "        \n",
    "        next_state = f.add_state()\n",
    "        f.add_arc(current_state, fst.Arc(in_label, eps, fst.Weight(weight_type, -math.log(0.1)), current_state))\n",
    "        if (isLast and i >= n):\n",
    "            f.add_arc(current_state, fst.Arc(in_label, out, fst.Weight(weight_type, -math.log(0.9)), next_state))\n",
    "        else:\n",
    "            f.add_arc(current_state, fst.Arc(in_label, eps, fst.Weight(weight_type, -math.log(0.9)), next_state))\n",
    "        \n",
    "        current_state = next_state\n",
    "    \n",
    "    return current_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_wfst(f, start_state, word, n):\n",
    "    \"\"\" Generate a WFST for any word in the lexicon, composed of n-state phone WFSTs.\n",
    "        This will currently output phone labels.  \n",
    "    \n",
    "    Args:\n",
    "        f (fst.Fst()): an FST object, assumed to exist already\n",
    "        start_state (int): the index of the first state, assumed to exist already\n",
    "        word (str): the word to generate\n",
    "        n (int): states per phone HMM\n",
    "        \n",
    "    Returns:\n",
    "        the constructed WFST\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    current_state = start_state\n",
    "    phone_list = lex[word]\n",
    "    for phone in phone_list[:-1]:\n",
    "        current_state = generate_phone_wfst(f, current_state, phone, n)\n",
    "    current_state = generate_phone_wfst(f, current_state, phone_list[-1], n, word, True)\n",
    "    f.set_final(current_state)\n",
    "    \n",
    "    return current_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_sequence_recognition_wfst(n = 3):\n",
    "    \"\"\" generate a HMM to recognise any sequence of words in the lexicon\n",
    "    \n",
    "    Args:\n",
    "        n (int): states per phone HMM\n",
    "\n",
    "    Returns:\n",
    "        the constructed WFST\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    f = fst.Fst(weight_type)\n",
    "    \n",
    "    # create a single start state\n",
    "    start_state = f.add_state()\n",
    "    f.set_start(start_state)\n",
    "    \n",
    "    N = len(lex.keys())\n",
    "    \n",
    "    for word in lex.keys():\n",
    "        new_start_state = f.add_state()\n",
    "        f.add_arc(start_state, fst.Arc(state_table.find(\"<eps>\"), phone_table.find(\"<eps>\"), fst.Weight(weight_type, -math.log(1/N)), new_start_state))\n",
    "        last_state = generate_word_wfst(f, new_start_state, word, n)\n",
    "        f.set_final(last_state)\n",
    "        f.add_arc(last_state, fst.Arc(state_table.find(\"<eps>\"), phone_table.find(\"<eps>\"), fst.Weight(weight_type, -math.log(1)), start_state))\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = parse_lexicon('lexicon.txt')\n",
    "word_table, phone_table, state_table = generate_symbol_tables(lex)  # we won't use state_table in this lab\n",
    "\n",
    "def generate_L_wfst(lex):\n",
    "    \"\"\" Express the lexicon in WFST form\n",
    "    \n",
    "    Args:\n",
    "        lexicon (dict): lexicon to use, created from the parse_lexicon() function\n",
    "    \n",
    "    Returns:\n",
    "        the constructed lexicon WFST\n",
    "    \n",
    "    \"\"\"\n",
    "    L = fst.Fst()\n",
    "    \n",
    "    # create a single start state\n",
    "    start_state = L.add_state()\n",
    "    L.set_start(start_state)\n",
    "    \n",
    "    for (word, pron) in lex.items():\n",
    "        \n",
    "        current_state = start_state\n",
    "        for (i,phone) in enumerate(pron):\n",
    "            next_state = L.add_state()\n",
    "            \n",
    "            if i == len(pron)-1:\n",
    "                # add word output symbol on the final arc\n",
    "                L.add_arc(current_state, fst.Arc(phone_table.find(phone), \\\n",
    "                                                 word_table.find(word), None, next_state))\n",
    "            else:\n",
    "                L.add_arc(current_state, fst.Arc(phone_table.find(phone),0, None, next_state))\n",
    "            \n",
    "            current_state = next_state\n",
    "                          \n",
    "        L.set_final(current_state)\n",
    "        L.add_arc(current_state, fst.Arc(0, 0, None, start_state))\n",
    "        \n",
    "    L.set_input_symbols(phone_table)\n",
    "    L.set_output_symbols(word_table)                      \n",
    "    \n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wfst(n = 3):\n",
    "    f = generate_word_sequence_recognition_wfst(n)\n",
    "    f.set_input_symbols(state_table)\n",
    "    f.set_output_symbols(word_table)\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyViterbiDecoder:\n",
    "    \n",
    "    NLL_ZERO = 1e10  # define a constant representing -log(0).  This is not really infinite, but approximate\n",
    "                     # it here with a very large number\n",
    "    \n",
    "    def __init__(self, f, audio_file_name):\n",
    "        \"\"\"Set up the decoder class with an audio file and WFST f\n",
    "        \"\"\"\n",
    "        self.om = observation_model.ObservationModel()\n",
    "        self.f = f\n",
    "        \n",
    "        if audio_file_name:\n",
    "            self.om.load_audio(audio_file_name)\n",
    "        else:\n",
    "            self.om.load_dummy_audio()\n",
    "        \n",
    "        self.initialise_decoding()\n",
    "    \n",
    "    def initialise_decoding(self):\n",
    "        \"\"\"set up the values for V_j(0) (as negative log-likelihoods)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.V = []\n",
    "        self.B = [] # B[t][i]\n",
    "        self.W = []\n",
    "        for t in range(self.om.observation_length()+1):\n",
    "            self.V.append([self.NLL_ZERO]*self.f.num_states())\n",
    "            self.B.append([-1]*self.f.num_states())\n",
    "            self.W.append(['']*self.f.num_states())\n",
    "        \n",
    "        # The above code means that self.V[t][j] for t = 0, ... T gives the Viterbi cost\n",
    "        # of state j, time t (in negative log-likelihood form)\n",
    "        # Initialising the costs to NLL_ZERO effectively means zero probability    \n",
    "        \n",
    "        # give the WFST start state a probability of 1.0   (NLL = 0.0)\n",
    "        start_state = self.f.start()\n",
    "        N = len(lex)\n",
    "        for arc in self.f.arcs(start_state):\n",
    "            s = arc.nextstate\n",
    "            self.V[0][s] = -math.log(1/N)\n",
    "\n",
    "        # some WFSTs might have arcs with epsilon on the input (you might have already created \n",
    "        # examples of these in earlier labs) these correspond to non-emitting states, \n",
    "        # which means that we need to process them without stepping forward in time.  \n",
    "        # Don't worry too much about this!  \n",
    "        self.traverse_epsilon_arcs(0)\n",
    "        \n",
    "    def traverse_epsilon_arcs(self, t):\n",
    "        \"\"\"Traverse arcs with <eps> on the input at time t\n",
    "        \n",
    "        These correspond to transitions that don't emit an observation\n",
    "        \n",
    "        We've implemented this function for you as it's slightly trickier than\n",
    "        the normal case.  You might like to look at it to see what's going on, but\n",
    "        don't worry if you can't fully follow it.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "#         states_to_traverse = list(range(self.f.num_states())) # traverse all states\n",
    "#         while states_to_traverse:\n",
    "            \n",
    "#             # Set i to the ID of the current state, the first \n",
    "#             # item in the list (and remove it from the list)\n",
    "#             i = states_to_traverse.pop(0)   \n",
    "        \n",
    "#             # don't bother traversing states which have zero probability\n",
    "#             if self.V[t][i] == self.NLL_ZERO:\n",
    "#                     continue\n",
    "        \n",
    "#             for arc in self.f.arcs(i):\n",
    "                \n",
    "#                 if arc.ilabel == 0:     # if <eps> transition\n",
    "                  \n",
    "#                     j = arc.nextstate   # ID of next state  \n",
    "                \n",
    "#                     if self.V[t][j] > self.V[t][i] + float(arc.weight):\n",
    "                        \n",
    "#                         # this means we've found a lower-cost path to\n",
    "#                         # state j at time t.  We might need to add it\n",
    "#                         # back to the processing queue.\n",
    "#                         self.V[t][j] = self.V[t][i] + float(arc.weight)\n",
    "                  \n",
    "#                         if j not in states_to_traverse:\n",
    "#                             states_to_traverse.append(j)\n",
    "\n",
    "        states_to_traverse = list(self.f.states()) # traverse all states\n",
    "        while states_to_traverse:\n",
    "            \n",
    "            # Set i to the ID of the current state, the first \n",
    "            # item in the list (and remove it from the list)\n",
    "            i = states_to_traverse.pop(0)   \n",
    "        \n",
    "            # don't bother traversing states which have zero probability\n",
    "            if self.V[t][i] == self.NLL_ZERO:\n",
    "                    continue\n",
    "        \n",
    "            for arc in self.f.arcs(i):\n",
    "                \n",
    "                if arc.ilabel == 0:     # if <eps> transition\n",
    "                  \n",
    "                    j = arc.nextstate   # ID of next state  \n",
    "                \n",
    "                    if self.V[t][j] > self.V[t][i] + float(arc.weight):\n",
    "                        \n",
    "                        # this means we've found a lower-cost path to\n",
    "                        # state j at time t.  We might need to add it\n",
    "                        # back to the processing queue.\n",
    "                        self.V[t][j] = self.V[t][i] + float(arc.weight)\n",
    "                        \n",
    "                        # save backtrace information.  In the case of an epsilon transition, \n",
    "                        # we save the identity of the best state at t-1.  This means we may not\n",
    "                        # be able to fully recover the best path, but to do otherwise would\n",
    "                        # require a more complicated way of storing backtrace information\n",
    "                        self.B[t][j] = self.B[t][i] \n",
    "                        \n",
    "                        # and save the output labels encountered - this is a list, because\n",
    "                        # there could be multiple output labels (in the case of <eps> arcs)\n",
    "                        if arc.olabel != 0:\n",
    "                            self.W[t][j] = self.W[t][i] + [arc.olabel]\n",
    "                        else:\n",
    "                            self.W[t][j] = self.W[t][i]\n",
    "                        \n",
    "                        if j not in states_to_traverse:\n",
    "                            states_to_traverse.append(j)\n",
    "\n",
    "    \n",
    "    def forward_step(self, t):\n",
    "        for i in self.f.states():\n",
    "            \n",
    "            if not self.V[t-1][i] == self.NLL_ZERO:   # no point in propagating states with zero probability\n",
    "                \n",
    "                for arc in self.f.arcs(i):\n",
    "                    \n",
    "                    if arc.ilabel != 0: # <eps> transitions don't emit an observation\n",
    "                        j = arc.nextstate\n",
    "                        tp = float(arc.weight)  # transition prob\n",
    "                        ep = -self.om.log_observation_probability(self.f.input_symbols().find(arc.ilabel), t)  # emission negative log prob\n",
    "#                         print(tp, ep, self.V[t-1][i])\n",
    "                        prob = tp + ep + self.V[t-1][i] # they're logs\n",
    "                        if prob < self.V[t][j]:\n",
    "                            self.V[t][j] = prob\n",
    "                            self.B[t][j] = i\n",
    "                            \n",
    "                            # store the output labels encountered too\n",
    "                            olbl = arc.olabel\n",
    "                            olbl = self.f.output_symbols().find(olbl)\n",
    "                            self.W[t][j] = olbl\n",
    "    \n",
    "    def finalise_decoding(self):\n",
    "        \n",
    "        # TODO - exercise\n",
    "        states = list(range(self.f.num_states()))\n",
    "        for i in states:\n",
    "            prob_final = float(self.f.final(i)) # probablity of being the end state (0 for non finals, and upwards of 1 if one final state, or split between all final states)\n",
    "            if (self.V[-1][i] < self.NLL_ZERO):\n",
    "                if (prob_final == math.inf): # not a final state\n",
    "                    self.V[-1][i] = self.NLL_ZERO\n",
    "                else: # is a final state\n",
    "                    self.V[-1][i] += prob_final # includes the weighting of ending at each of the final states in the path towards them in the last step\n",
    "        \n",
    "    def decode(self):\n",
    "        \n",
    "        self.initialise_decoding()\n",
    "        t = 1\n",
    "        while t <= self.om.observation_length():\n",
    "            self.traverse_epsilon_arcs(t)\n",
    "            self.forward_step(t)\n",
    "            self.traverse_epsilon_arcs(t)\n",
    "            t += 1\n",
    "        \n",
    "        self.finalise_decoding()\n",
    "    \n",
    "    def backtrace(self):\n",
    "        \n",
    "        # TODO - exercise \n",
    "        \n",
    "        # complete code to trace back through the\n",
    "        # best state sequence\n",
    "        \n",
    "        # You'll need to create a structure B_j(t) to store the \n",
    "        # back-pointers (see lectures), and amend the functions above to fill it.\n",
    "        \n",
    "#         for w in self.W:\n",
    "#             print(dict(enumerate(w)))\n",
    "\n",
    "        # --------------\n",
    "\n",
    "#         T = self.om.observation_length()\n",
    "#         current = -1\n",
    "#         for i in range(self.f.num_states()):\n",
    "#             if (float(self.f.final(i)) != math.inf):\n",
    "#                 if (self.B[T][i] != -1):\n",
    "#                     current = i\n",
    "#                     break\n",
    "#         if (current == -1):\n",
    "#             raise Exception('No valid path')\n",
    "#         seq = [current]\n",
    "#         currentStr = ''\n",
    "#         strSeq = ''\n",
    "        \n",
    "#         for t in range(T,-1, -1):\n",
    "#             tmpStr = self.W[t][current]\n",
    "#             if not(tmpStr in ['', '<eps>',  currentStr]):\n",
    "#                 currentStr = tmpStr\n",
    "#                 strSeq = f'{currentStr} {strSeq}'\n",
    "        \n",
    "       \n",
    "#             current = self.B[t][current]\n",
    "#             seq.insert(0, current)\n",
    "        \n",
    "        \n",
    "#         strSeq = strSeq.strip()\n",
    "#         best_state_sequence = (seq, strSeq)\n",
    "        \n",
    "#         return best_state_sequence\n",
    "\n",
    "\n",
    "        # --------------------------------------------------\n",
    "        T = self.om.observation_length()\n",
    "        current = -1\n",
    "        for i in range(self.f.num_states()):\n",
    "            if (float(self.f.final(i)) != math.inf): # if i is final\n",
    "                if (self.B[T][i] != -1):\n",
    "                    if (current == -1):\n",
    "                        current = i\n",
    "                    elif (self.V[-1][current] > self.V[-1][i]):\n",
    "                        current = i\n",
    "        if (current == -1):\n",
    "            raise Exception('No valid path')\n",
    "        seq = [current]\n",
    "        currentStr = ''\n",
    "        strSeq = ''\n",
    "        \n",
    "        for t in range(T,-1, -1):\n",
    "            tmpStr = self.W[t][current]\n",
    "            if not(tmpStr in ['', '<eps>']):\n",
    "#             if True:\n",
    "                currentStr = tmpStr\n",
    "                strSeq = f'{currentStr} {strSeq}'\n",
    "        \n",
    "       \n",
    "            current = self.B[t][current]\n",
    "            seq.insert(0, current)\n",
    "        \n",
    "        \n",
    "        strSeq = strSeq.strip()\n",
    "        best_state_sequence = (seq, strSeq)\n",
    "        \n",
    "        return best_state_sequence\n",
    "    \n",
    "    # --------------------------------------------------\n",
    "\n",
    "#         best_final_state = self.V[-1].index(min(self.V[-1])) # argmin\n",
    "#         best_state_sequence = [best_final_state]\n",
    "#         best_out_sequence = []\n",
    "\n",
    "#         t = self.om.observation_length()   # ie T\n",
    "#         j = best_final_state\n",
    "\n",
    "#         while t >= 0:\n",
    "#             i = self.B[t][j]\n",
    "#             best_state_sequence.append(i)\n",
    "#             best_out_sequence = [self.W[t][j]] + best_out_sequence  # computer scientists might like\n",
    "#                                                                                 # to make this more efficient!\n",
    "\n",
    "#             # continue the backtrace at state i, time t-1\n",
    "#             j = i  \n",
    "#             t-=1\n",
    "\n",
    "#         best_state_sequence.reverse()\n",
    "\n",
    "#         # convert the best output sequence from FST integer labels into strings\n",
    "# #         print(best_state_sequence)\n",
    "# #         print([label for label in best_out_sequence])\n",
    "# #         print([self.f.output_symbols().find(label) for label in best_out_sequence])\n",
    "#         best_out_sequence = ' '.join([ label for label in best_out_sequence if not(label in ['', '<eps>'])])\n",
    "# #         best_out_sequence = ' '.join([ self.f.output_symbols().find(label) for label in best_out_sequence if not(label in ['', '<eps>'])])\n",
    "\n",
    "#         return (best_state_sequence, best_out_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_transcription(wav_file):\n",
    "    \"\"\"\n",
    "    Get the transcription corresponding to wav_file.\n",
    "    \"\"\"\n",
    "    \n",
    "    transcription_file = os.path.splitext(wav_file)[0] + '.txt'\n",
    "    \n",
    "    with open(transcription_file, 'r') as f:\n",
    "        transcription = f.readline().strip()\n",
    "    \n",
    "    return transcription\n",
    "\n",
    "def run():\n",
    "    f = create_wfst()\n",
    "    \n",
    "    files = list(glob.glob('/group/teaching/asr/labs/recordings/*.wav'))\n",
    "    \n",
    "    # clear results file\n",
    "    file = open('results.txt', 'w')\n",
    "    file.write('')\n",
    "    file.close()\n",
    "    \n",
    "#     for i in range(len(files)):\n",
    "#         wav_file = files[i]\n",
    "#         transcription = read_transcription(wav_file)\n",
    "#         print(f'{i}: {transcription}')\n",
    "#     return\n",
    "\n",
    "    # 10:15\n",
    "    # 37:42\n",
    "#     for wav_file in files[10:15]:    # replace path if using your own audio files\n",
    "    for wav_file in files:    # replace path if using your own audio files\n",
    "        decoder = MyViterbiDecoder(f, wav_file)\n",
    "        \n",
    "        decoder.decode()\n",
    "#         print(decoder.V)\n",
    "#         input()\n",
    "        (state_path, words) = decoder.backtrace()  # you'll need to modify the backtrace() from Lab 4\n",
    "                                                   # to return the words along the best path\n",
    "\n",
    "\n",
    "        transcription = read_transcription(wav_file)\n",
    "        text = (f\"\\n\\n{state_path}\\n\\n[{words}]\\n[{transcription}]\")\n",
    "        error_counts = wer.compute_alignment_errors(transcription, words)\n",
    "        word_count = len(transcription.split())\n",
    "\n",
    "        res = '{error_counts} {word_count}'     # you'll need to accumulate these to produce an overall Word Error Rate\n",
    "        print(f'{words}\\n{transcription}\\n{res}\\n')\n",
    "        add_to_file('results.txt', f'{text}\\n{res}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = create_wfst()\n",
    "draw_f(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_75741/4218983175.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwer_elem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwer_counts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mwer_elem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwer_elem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"(\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\")\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mS\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwer_elem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mD\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwer_elem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mI\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwer_elem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "wer_counts = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "S, D, I, N = 0, 0, 0, 0\n",
    "wer_counts = wer_counts.split(\"\\n\")\n",
    "IndexCount = len(wer_counts)\n",
    "for wer_elem in wer_counts:\n",
    "    wer_elem = wer_elem.replace(\"(\", \"\").replace(\")\", \"\").replace(\",\", \"\").split(\" \")\n",
    "    S += int(wer_elem[0])\n",
    "    D += int(wer_elem[1])\n",
    "    I += int(wer_elem[2])\n",
    "    N += int(wer_elem[3])\n",
    "\n",
    "WER = (S + D + I) / N\n",
    "print(IndexCount, WER, S, D, I, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_to_file('test.txt', 'test1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
